---
title: 'Week 11/12'
date: 2023-11-27
permalink: /posts/2023/11/blog-post-11/
tags:
  - research
  - my honors project
---

This is the eleventh blog post for my honors thesis. Lots of results linked.

# Contents

- [Tasks Completed](#tasks)
- [Binary Classification](#binary)
- [Multiple Classification](#multiple)
- [Timeline](#calendar)
- [Goals](#moving)


---


<a name="tasks"></a>
# Tasks Completed 
- Did Binary Classification & Have Results for Multiple datasets
  - Pima-Indians (**done before**), Haberman's Survival Dataset, Sonar, Porto Seguro
- Performed and mostly implemented code to do Multiple Classification, but have some questions before going further
  - Glass, thyroid, wine (**done before, but I made it binary before**), E-Coli
- Overall: Eight Datasets (four binary, four multiple)
- Completed Junior Fall HNR 4998 Student Report 



---


<a name="binary"></a>
# Binary Classification

Four Datasets for Imbalanced Binary Classification were done:
- Pima-Indians (*this was done before this week*)
- Haberman's Survival Dataset
- Sonar
- Porto Seguro (currently training)

Four Classifiers were used to report results:
- Random Forest (averaged over thirty random forest classifiers)
- XGBoost
- Linear Discriminant Analysis (LDA)
- Logistic Regression (LR)

The results (and their violin plots) can be viewed [here](https://docs.google.com/document/d/1MjtnC1FCx61PJLp4J25m52Z3pYPaRkOBMU0m0PKqzaE/edit?usp=sharing). 

## Pima-Indians Results
For the random forests: 
  - The accuracy decreased by 0.0031249999999998224
  - The F1-Score increased by 0.038395585169522795
    
For XGBoost:
  - The accuracy increased by 0.02083333333333337
  - The F1-Score increased by 0.057534906114588336
    
For LR:
  - The accuracy decreased by 0.04166666666666674
  - The F1-Score increased by 0.007219251336898491
    
For LDA:
  - The accuracy decreased by 0.04166666666666674
  - The F1-Score increased by 0.002170963364993228


## Haberman's Survival Dataset
For the random forests: 
  - The accuracy decreased by 0.028571428571428248
  - The F1-Score increased by 0.17227668871790147

For XGBoost:
  - The accuracy decreased by 0.07792207792207795
  - The F1-Score increased by 0.13362864525655233
 
For LR:
  - The accuracy decreased by 0.06493506493506496
  - The F1-Score increased by 0.28913260219341974

For LDA:
  - The accuracy decreased by 0.06493506493506496
  - The F1-Score increased by 0.24011299435028247


## Sonar
For the random forests: 
  - The accuracy increased by 0.017948717948717996 
  - The F1-Score increased by 0.026978138978138944
    
For XGBoost:
  - The accuracy increased by 0.03846153846153855 
  - The F1-Score increased by 0.03418803418803429
    
For LR:
  - The accuracy decreased by 0.019230769230769384 
  - The F1-Score decreased by 0.02784503631961266
    
For LDA:
  - The accuracy increased by 0.019230769230769162 
  - The F1-Score increased by 0.022361984626135478

## Porto Seguro
**This TVAE is currently training**




<a name="multiple"></a>
# Multiple Classification
Four Datasets for Imbalanced Binary Classification were done:
- Wine (**now done with multiple classes, before was binary**)
- Thyroid
- Glass
- E-Coli

A couple notes:
- How do I calculate the F1-Score for these datasets because there are multiple minority classes? I don't have exact numbers.
- How do I handle what I like to call super-minority classes. For example, some of the E-Coli Classes only have ~4 observations when training, leading to errors when I calculate the k-nearest-neghbors. 
- The confusion matrices don't look great, they're ok. I don't want to do anything before seeing the numbers, but how could I improve imbalanced learning on multiple classification. Time to hit the books!




<a name="calendar"></a>
# Calendar

| Week | Days    | Content    | Checked |
| :---:   | :---: | :---: | :---: |
| Eight | October 24 - 30   | Research Linear Interpolation Methods   | **Complete** | 
| Nine | October 31 - November 6   | Implement Linear Interpolation Methods   | **Complete** |
| Ten | November 7 - 13   | Make Code Good/Reproduceable   | **Complete** |
| Eleven | November 14 - 20   | Find New Datasets and Get Results  | **Complete** |
| Twelve | November 21 - 27   | TBD  | |
| Thirteen | November 28 - December 4   | TBD   | |





<a name="questions"></a>
# Questions Raised
- How should I write the reports for the paper summaries. Length?


<a name="moving"></a>
# Moving Forward

## Short-Term Goals
- 


## Long-Term Goals
- Stay on the timeline
- Write up the first section of this paper?
