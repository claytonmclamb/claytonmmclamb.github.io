---
title: 'Week Eight'
date: 2023-10-30
permalink: /posts/2023/10/blog-post-9/
tags:
  - research
  - my honors project
---

This is the ninth blog post for my honors thesis. Good results below!

# Contents

- [Tasks Completed](#tasks)
- [Finished the Initial Application to NCUR](#ncur)
- [Got Results](#results)
- [Timeline](#calendar)
- [Goals](#goals)


---


<a name="tasks"></a>
# Tasks Completed 
- Finished the initial travel application to NCUR


---


<a name="ncur"></a>
# NCUR
This project addresses the critical issue of bias in machine learning and explores the potential of generative models to enhance fairness in classification models. Bias in predictive models arise from inadequate representation and sampling biases within training data, often resulting in discrimination against minority groups within the data. Traditionally, modeling and generating new tabular data has been a difficult challenge due to the variety of data types and conditional dependencies for each feature. With advances in generative technology, particular with the development of CTGAN/TVAE, we investigate the uses of variational autoencoders (VAE) to mitigate bias within a classification model. Interpolating and extrapolating new observations in the latent space of a TVAE, new and realistic observations for the minority class can be generated. Across a variety of classification algorithms, such as the random forest (RF) and XGBoost (XGB) models, overall accuracy was maintained and increased for the minority group. Across all classification models tested, the F1-Score of the model was increased, including a .087 increase in the XGB classifier. When compared with standard techniques to increase fairness in classification models, such as SMOTE, the techniques presented within this study remain competitive or improve existing research.


<a name="results"></a>
# Results from Latent Techniques Across Different Classifiers
- The results can be seen [here](https://docs.google.com/document/d/1YzyX2dl63IMxpiM6WksmRyKlZakcHYzvTWFjq10WMac/edit?usp=sharing).
- The Interpolation and Extrapolation techniques when k=3 appear competitive to vanilla SMOTE
- Will apply to more datasets
- Will research more techniques (DANN)


<a name="calendar"></a>
# Calendar

| Week | Days    | Content    | Checked |
| :---:   | :---: | :---: | :---: |
| Eight | October 24 - 30   | Research Linear Interpolation Methods   | **Complete** | 
| Nine | October 31 - November 6   | Implement Linear Interpolation Methods   | **Complete** |
| Ten | November 7 - 13   | Research Geometric Interpolation Methods   | |
| Eleven | November 14 - 20   | Implement Geometric Interpolation Methods  | |
| Twelve | November 21 - 27   | Compare results from all linear and geometric using KS-test, t-test, etc.  | |
| Thirteen | November 28 - December 4   | Write a report on the topic discussed above, fill out end-of-semester report, etc.   | |





<a name="questions"></a>
# Questions Raised
- Tried reading Discriminative Adaptive Nearest Neighbors, but how does it work?

<a name="moving"></a>
# Moving Forward

## Short-Term Goals
- Continue to work on geometric/nearest-neighbors methods?


## Long-Term Goals
- Stay on the timeline
- Write up the first section of this paper?
