---
title: 'Week Three'
date: 2023-09-11
permalink: /posts/2023/09/blog-post-1/
tags:
  - research
  - my honors project
---

This is the third blog post for my research project. It contains progress from September 11 to Setptember 17. 

# Contents

- [Tasks Completed](#tasks)
- [Papers Read](#papers)
- [Synthetic Data and Decision Trees](#trees)
- [PCA (again)](#pca)
- [Neural-Net prediciting Real vs. Synthetic Data](#net)
- [Questions](#questions)
- [Moving Forward](#moving)

---


<a name="tasks"></a>
# Tasks Completed 
- Read four papers
  - Decision trees
  - Bias
  - Application of CTGAN and TVAE
  - FairGAN
- Finished annotated bibliography
- Found a cool phenomenon with synthetic data and decision trees (for a simple dataset)
- Re-did PCA. This time it includes both TVAE and CTGAN, in addition to alpha
- Built a neural net wiuth 25 hidden layers to attempt to predict real and synthetic data, results documented

---

<a name="papers"></a>
# Papers Read

## $\mathcal{\color{purple}{Decision \ trees: \ a \ recent \ overview}}$
[Link](https://link.springer.com/article/10.1007/s10462-011-9272-4)

Compared to deep-learning architectures and models, decision trees offer a high level of explainability; decision trees demonstrate a series of logical conditions and are often able to generalize well and have accuracy as high as deep learning models. Developing in two phases, decision trees first grow based on finding an optimal splitting between classes, then pruning the tree in attempt to limit over-fitting. Decision trees are non-trivial, and their optimization has proven to be NP-hard. Tree structure and accuracy are highly sensitive to the training data given, methods such as bagging and boosting have been developed to improve decision tree accuracy with smaller training data. Bagging uses an ensemble of trees based on different subsets of training data, while boosting builds an ensemble of trees based on observations that have not been correctly learned. This project will primarily focus on decreasing bias and improving accuracy in classification, where decision trees are often the de-facto data mining technique.  

## $\mathcal{\color{purple}{Synthetic \ Data \ Guided \ Breast \ Cancer \ Diagnosis \ and \ Prognosis \ Using \ Integrated \ Deep \ Framework}}$
[Link](https://ssrn.com/abstract=4014276)

Breast cancer diagnosis, in conjunction with machine and deep-learning algorithms, lead to a one percent decrease in mortality each year from 2013 to 2018. To continue to improve AI methods within the medical domain, more training data is needed. Synthetic data allows researchers to build datasets both high in quantity and quality for their models. To test the potential of diagnostic and prognostic capabilities of synthetic data, the authors used the UCI Wisconsin Breast Cancer Diagnostic Dataset and two models, CTGAN (Conditional Tabular Generative Adversarial Network) and TVAE (Tabular Variational Autoencoder). Evaluating the architectures generating synthetic data using the Kolmogorov-Smirov test and Chi-squared test, the authors concluded that the TVAE architecture produced higher quality synthetic data. In addition to the statistical tests performed, models trained using data generated from the TVAE performed better than samples generated from the CTGAN architecture. This research demonstrates the validity, ability, and importance of generative models, in addition to providing metrics to determine the quality of synthetic data.  

---

<a name="trees"></a>
# Synthetic Data and Decicion Trees Experiment

I already showed you this, but the images [here](https://github.com/claytonmclamb/claytonmmclamb.github.io/blob/master/images/generative_findings_ctgan'.png) anmd [here](https://github.com/claytonmclamb/claytonmmclamb.github.io/blob/master/images/generative_findings_tvae.png) demonstrate the ability for the models to generalize better using only synthetic data. They follow, a similar-logarithmic pattern. 

---

<a name="pca"></a>
# PCA (again)

In week two, I did PCA on CTGAN. After learning how to use TVAE and I should have used alpha, I remade the graphs and they had interesting results. Click [here for CTGAN](https://github.com/claytonmclamb/claytonmmclamb.github.io/blob/master/images/pca_ctgan.png) and [here for TVAE](https://github.com/claytonmclamb/claytonmmclamb.github.io/blob/master/images/pca_tvae.png). It appears that the spread for TVAE is much tighter than CTGAN, which hints at it generating better data. 

---


<a name="net"></a>
# Neural Net Predicting Real & Synthetic Data

I built a neural network in PyTorch wiht 25 hidden layers to try and learn the difference between real and synthetic data. The results for [CTGAN](https://github.com/claytonmclamb/claytonmmclamb.github.io/blob/master/images/net_preds_ctgan.png) and [TVAE](https://github.com/claytonmclamb/claytonmmclamb.github.io/blob/master/images/net_preds_tvae.png) can be seen by clicking on the links. In both models, the neural network failed to discrimate. However, TVAE performed ever-so-slightly better.

---


<a name="questions"></a>


---
# Questions

- How "statistic-y" should/can my proposal sound? *If it sounds like alot of staistics I can waive my statistics practicum*. How can I make it sound more sound in statistics? (talk about gaussian distribition, chi-squared tests, the one non-parametric test with the Russian guy's name, PCA)
-  How much should I put about my experminets with the wine quality dataset? I like the graphs/experiments I made and think they're super cool, but I don't know where I could/should put them. Is it acceptable to talk about wine-quality in a research proposal? It was the dataset that required the least amount of cleaning!
-  I read "methods that target biases in the algorithms fall under three categories: pre-processing, in-processing, and post-processing." I have thought a lot about the pre-processing and in-processing portions. For pre-processing, we could combine CTGAN and FairGAN (which a paper has done, but I haven't read it) to generate unbiased data. For in-processing, I have though about using boosting and the TVAE to generate samples trees struggle with, eventually building an ensemble of unbiased trees.
-  After talking to ChatGPT about what I should call my project, it came up with the title "**BOOST: Building Optimal, Objective, and Synthetic Trees**." Not really a question, but I think that sounds good for the time being. 



---




<a name="moving"></a>
# Moving Forward

## Short-Term Goals
## Long-Term Goals


---
