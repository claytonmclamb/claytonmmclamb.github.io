---
title: 'Week Three'
date: 2024-02-20
permalink: /posts/2024/02/blog-post-14/
tags:
  - research
  - my honors project
---

This is the twelth blog post for my honors thesis. A working VAE!

# Contents

- [Tasks Completed](#tasks)
- [Timeline](#calendar)
- [Goals](#moving)


---


<a name="tasks"></a>
# Tasks Completed 
- Read about VAEs from that really good fellow, Goodfellow
- Asked ChatGPT to make a VAE for tabular data
  - The ChatGPT VAE worked, but did not reconstruct very well across classes
- Fixed the reconstruction using methods from previous VAE's
- Found new ideas to implement
- Outlined next steps


# VAE made by Chat GPT
I asked chat GPT to make a variational autoencoder. 

After making slight edits for it to work and train, I got a working VAE that does not have the strange posterior collapse/vector problem.

The issue with this VAE is that it did not reconstruct across classes very well. It would kind of place data in the middle/around the mean of all the classes. 

# Fixing the Class Reconstruction Issue

From previous VAE's I learned that sending the classes in as seperate batches worked particularly well for each class. 

After attempting this mini-batch solution, it appears to work.

*Note: extensive testing has not been done yet to confirm this, but I feel fairly confident after looking at plots*

# Denoising VAES?

After doing some research, I stumbled upon a Kaggle competition for tabular data, where users made denoising Autoencoders. 

I think this could be an interesting addition to the architecture. The best noise they found was row swapping, where we would take that idea and implement it within each minority/majority class. 

Apparently this extracts a (hopefully) better representation of these features.

Could be similar to image augmentation. Additionally, sounds rather logically sound for augmenting minority samples for training (which could make a better model). 

# Approximating the size of the hidden/latent space?

How do people choose the dimensions of their hidden/latent space? Do they?

I had a shower thought where we use PCA and find where a certain threshold of the variance is explained by n components. Using this number, n, we can make our model. 





<a name="calendar"></a>
# Calendar

*What we're gonna plan*

| Week | Days    | Content    | Checked |
| :---:   | :---: | :---: | :---: |





<a name="questions"></a>
# Questions Raised
- Should I apply to SURF?

<a name="moving"></a>
# Moving Forward
- *Also a point of discussion*

## Short-Term Goals
- Implement KNNOR


## Long-Term Goals
- Apply to SURF?
