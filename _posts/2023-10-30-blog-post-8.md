---
title: 'Week Eight'
date: 2023-10-30
permalink: /posts/2023/10/blog-post-8/
tags:
  - research
  - my honors project
---

This is the eighth blog post for my honors thesis. Good results below!

# Contents

- [Tasks Completed](#tasks)
- [A Review of Linear Methods in the Latent Space Survey](#latent)
- [Augmentation of Indian Diabetes](#diabetes)
    - Training the TVAE w/o the target Variable
    - Augmentating Data w/ a **very impressive violin plot**
    - Results from a classification model
- [NCUR](#ncur)
- [Timeline](#calendar)
- [Goals](#goals)


---


<a name="tasks"></a>
# Tasks Completed 
- Reviewed the linear methods in the latent space synthetic survey
- Performed Linear Interpolation
  - Trained new autoencoder w/o target variable
  - Find the k-nearest neighbors of each compressed observation
  - Augment new data using linear interpolation
- Reported Results from Linear Interpolation/Augmentation
  - Made a really cool violin plot
  - Built classifier
  - Performed the KS-Test for continuous columns
- Worked on the abstract for NCUR


---


<a name="latent"></a>
# Review of Linear Methods in Latent Survey
Two types of linear transformations: Between and within class transformations
- Within: Chooses two observations from the same class
- Between: Chooses two observations from different classes
Both within and between class interpolation follow this general equation
- xs= xj+ (xi-xj)
  
However, there are multiple adjustments to this general equation.
Such as the observation-based linear extrapolation mechanism:
- xs= xj+ (xj-xi)
  
Or the hard extrapolation method:
- xs= xj+ (xj - c) where c is the mean of a classes observations
  
Generally, most SMOTE-based methods select a center observation and a random observation within its k-nearest neighbors belonging to the same class, while the mixup method selects two random observations regardless of class membership. In addition to the methods discussed above, you can combine interpolation and extrapolation methods and apply something called a “difference transform.” [This]() image shows these methods.


<a name="diabetes"></a>
# Augmentation Techniques
In the last meeting, we discussed training the TVAE without access to the target variable. Training the TVAE without the target variable and interpolating in the latent space leads us to assume that the synthetic observations belong to the class they were interpolated from. 

For each observation within the latent space, I found their three closest neighbors. After applying KNN, I randomly selected an observation and randomly selected one of their closest neighbors. The results from this interpolation technique can be seen below. Improvements to this algorithm could be:
- Incorporating extrapolation
- Incorporating a probability model that chooses more central observations more often


<a name="results"></a>
# Results from Augmentation Techniques
## Violion Plot
The assymetric violin plot compares real and synthetic distributions across each feature. The plot can be seen [here]().
The violin plot demonstrates that the augmentation technique discussed above has a good ability to model the real distribution.

## Statistical Tests
The KSTest provides insight into how well the synthetic data models continous data. The results from the KSTest for each column can be seen below.

| Column | Type    | Test    | P-Value |
| :---:   | :---: | :---: | :---: |
| Diabetes Pedigree | Continuous   | KS-Test   | 0.3276 | 
| BMI | Continuous   | KS-Test   | 0.0157 |

The model modeled diabetes pedigree very well and BMI "less well," but depending on your level of significance you can reject the null hypothesis. **In the future, I will explore the CS-Test for discrete data, but could not get it to work prior to this week.**

## Classification Models


<a name="ncur"></a>
# NCUR




<a name="calendar"></a>
# Calendar

| Week | Days    | Content    | Checked |
| :---:   | :---: | :---: | :---: |
| Eight | October 24 - 30   | Research Linear Interpolation Methods   | **Complete** | 
| Nine | October 31 - November 6   | Implement Linear Interpolation Methods   | |
| Ten | November 7 - 13   | Research Geometric Interpolation Methods   | |
| Eleven | November 14 - 20   | Implement Geometric Interpolation Methods  | |
| Twelve | November 21 - 27   | Compare results from all linear and geometric using KS-test, t-test, etc.  | |
| Thirteen | November 28 - December 4   | Write a report on the topic discussed above, fill out end-of-semester report, etc.   | |





<a name="questions"></a>
# Questions Raised
- 

<a name="moving"></a>
# Moving Forward

## Short-Term Goals
- Continue to work on linear methods
- Appply to NCUR


## Long-Term Goals
- Apply to NCUR
- Stay on the timeline.
