---
title: 'Week Two'
date: 2023-09-11
permalink: /posts/2023/09/blog-post-2/
tags:
  - research
  - my honors project
---

This is the second blog post for my research project. If you're looking for the first, it was done in RMarkdown and won't be here...yet. 

# Contents

- [Tasks Completed](#tasks)
- [Papers Read](#papers)
- [Implementation of CTGAN](#ctgan)
- [Questions](#questions)
- [Moving Forward](#moving)

---

<a name="tasks"></a>
# Tasks Completed 
- Read Three Papers
   - Tabular and Latent Space Syntehtic Data Generation: a Literature Review
   - Modeling Tabular Data using Conditional GAN
   - A Survey of Deep Active Learning
- Got this website to work for this and future blog posts
- Got CTGAN implemented on the Wine Quality dataset

---

<a name="papers"></a>
# Papers Read

## $\mathcal{\color{purple}{Tabular \ and \ Latent \ Space \ Synthetic \ Data \ Generation: \ a \ Literature \ Review}}$
[*Link*](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-023-00792-7#:~:text=This%20literature%20review%20focuses%20on%20generation%20mechanisms%20applied,data%20and%20latent%20space%20may%20be%20used%20interchangeably.)

Synthetic data, acquired through generative processes that attempt to mimic real data, is studied infrequently for tabular data when compared to images or text. In an attempt to provide a literature review for tabular synthetic data, Fonseca and Bacao review over 70 generation algorithms to focus on the core generative process of synthetic tabular data. Proposing a taxonomy for synthetic data generation, consisting of the architecture, application level, data space, and scope, Fonseca and Bacao are able to assess the success of different levels across different applications. When generating data, the context of the application determines how the data will be generated. For example, when focusing on the anonymization of data, a model based on differential privacy would be employed. In addition to privacy, regularization, oversampling, active learning, semi-supervised learning, and self-supervised learning are all applications reviewed. Concluding with a review of generation mechanisms, such as linear or geometric transformations, and novel evalutaion metrics, the authors were able to provide a unique and comprehensive survey for tabular synthetic data generation. This research will rely on the implementation of synthetic data generation models, such as CTGAN or TVAE, and this literature review provided invaluable insight into and organization for generative algorithms.  

## $\mathcal{\color{purple}{Modeling \ Tabular \ Data \ using \ Conditional \ GAN}}$
[*Link*](https://dl.acm.org/doi/10.5555/3454287.3454946)

Tabular data generation, an attempt to generate synthetic observations consisting of both continuous and discrete data, is a difficult task. In this paper, researchers developed CTGAN, an adversarial based generative model to produce high quality synthetic observations regardless of multiple modes within continuous distributions and imbalance among discrete columns. Typically, mixed data types, non-Gaussian distributions, multimodal distributions, one-hot encoded vectors, and imbalanced categories present massive difficulties for normal generative architectures and results in subpar synthetic observations. For continuous data, CTGAN utilizes a variational gaussian mixture model to identify the mode an observation will belong to. For discrete data, a conditional vector is used to indicate discrete values and one-hot encoded vectors. Using these methods, these researchers were able to develop CTGAN (based on a generative adversarial network) as well as TVAE (based on a variational autoencoder). Overall, TVAE and CTGAN are able to provide much more realistic synthetic observations, performing better than previous generative models such as CLBN and MedGAN. 

## $\mathcal{\color{purple}{A \ Survey \ of \ Deep \ Active \ Learning}}$
[*Link*](https://dl.acm.org/doi/10.1145/3472291)

*Have not finished this survey (roughly 2/3 done), because it brought up a very interesting point about synthetic data and active learning*

This survey discusses the combination of deep learning models and active learning. While deep learning focuses on the automatic extraction of features, active learning focuses on query learning. The central question driving active learning is given a dataset of unlabeled examples, what are the observations that would provide the most needed information to the model. These observations can be chosen based on uncertainty, diversity, quantity, and other methods. 



---

<a name="questions"></a>
# Questions

- If we were investigating the improvement of models using active learning and generative models (*specifically applied to tabular data*), would the generated observations not be very beneficial because they are based on real observations (the Survey on Deep Active Learning mentioned this)? Or would the random-discrete-ness of CTGAN (randomly choosing discrete values) prevent this?


---

<a name="ctgan"></a>

# Implementation of CTGAN

Looking for a dataset that is completely numeric (some continous and discrete values), the Wine Quality dataset was used (was considering UCI Adult, but the data cleaning was too much for now). Following [this](https://www.kdnuggets.com/2022/03/generate-tabular-synthetic-dataset.html) tutorial (some things have been slightly changed, I generated 200 samples. I specified "quality" as the only discrete column, and it worked! Results from the table and PCA are shown [here](https://github.com/claytonmclamb/claytonmmclamb.github.io/blob/master/images/Screenshot%202023-09-10%20220444.png) and [here](https://github.com/claytonmclamb/claytonmmclamb.github.io/blob/master/images/pca_ctgan.png). (*these blog posts may not support images being included yet*)

The graph above demonstrates real observations in blue and synthetic observations in red. By compressing the data down into two principal components, the data generated appears to be very similar to the real observations only after ten epochs. 

---




<a name="moving"></a>
# Moving Forward

## Short-Term Goals
- Have at least ten sources read & annotated by next week (9/18)
  - *currently have six sources*
  - Read another on tabular data generation (preferabbly CTGAN/TVAE)
  - Read another on active learning for tabular-based problems (lots for computer-vision, text, not for tabular data)
  - Read one on the advantages and disadvantages of generative modeling
- Try and make some basic visuals to make these topics digestible (*can be used for proposal*)
- Finalize the project (*active learning, fair learning, etc.*)

## Long-Term Goals
- Have the narrative and proposal *done* by the rough draft deadline on 9/28
  - This includes an abstract, introduction, literature review, main goal, methods, final product, annotated bib, budget, timeline
  - Gives until 10/18 to implement recomennded changes, finalize the timeline and  budget, get signatures, etc. (*I thought the deadline was 10/28, but still on track to finish*)
  - **this goal has not changed from last week**


---
